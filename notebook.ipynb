{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nanom/llm_adaptation_2nlp_workshop/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65c9fdf3",
      "metadata": {
        "id": "65c9fdf3"
      },
      "source": [
        "# Taller: Modelos de Lenguaje a tu medida ü§ñüí¨\n",
        "> ### Adaptaci√≥n de dominio de modelos de lenguaje pre-entrenados\n",
        "> ### <i>2do Workshop Argentino de NLP - C√≥rdoba 2023</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32a802c2",
      "metadata": {
        "id": "32a802c2"
      },
      "source": [
        "¬øQui√©nes somos?\n",
        "------------------\n",
        "\n",
        "* **Hern√°n Maina** - Becario doctoral de CONICET ( [ü§ó](https://huggingface.co/nanom) | [üåê](https://nanom.github.io/) | [üì´](mailto:hernan.maina@mi.unc.edu.ar))\n",
        "* **Guido Ivetta** - Becario doctoral de V√≠a Libre y Profesor Ayudante de FAMAF ([ü§ó](https://huggingface.co/guidoivetta) | [üì´](mailto:guidoivetta@mi.unc.edu.ar ))\n",
        "* **Laura Alonso Alemany** - Profesora Asociada de FAMAF ([üê¶](@morlaicassiopea) | [üåê](https://www.cs.famaf.unc.edu.ar/~laura/) | [üì´](mailto:lauraalonsoalemany@unc.edu.ar))\n",
        "* **Luciana Benotti** - Profesora Asociada de FAMAF ([üê¶](@LucianaBenotti) | [üåê](https://benotti.github.io/) | [üì´](mailto:luciana.benotti@unc.edu.ar))\n",
        "\n",
        "<div style=\"text-align:center\">\n",
        " <h4 style=\"font-size:1.5em;margin:5px\"></h4>\n",
        "    <h5 style=\"font-style:normal;font-size:1em;margin:5px\"></h5>\n",
        "    <div style=\"display:inline-block;margin-right:0px;\">\n",
        "        <img src=\"https://vialibre.org.ar/wp-content/uploads/2020/10/banner-2.jpg\" style=\"height:10em;width:auto;\"/>\n",
        "    </div>\n",
        "    <h6 style=\"font-style:normal;font-size:0.9em;margin:5px;\">\n",
        "        <a href=\"https://twitter.com/\" style=\"color:royalblue;\" target=\"_blank\"> @fvialibre</a> -\n",
        "        <a href=\"https://www.vialibre.org.ar/\" style=\"color:royalblue;\" target=\"_blank\">https://www.vialibre.org.ar/</a>\n",
        "    </h6>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb685a55",
      "metadata": {
        "id": "bb685a55"
      },
      "source": [
        "## Esquema del taller\n",
        "\n",
        "1. [Modelos de lenguaje](#Modelos-de-lenguaje)\n",
        "2. [Librerias](#Librerias)\n",
        "3. [Probemos modelos](#Probemos-modelos)\n",
        "4. [Personalicemos un modelo](#Personalicemos-un-modelo)\n",
        "\n",
        "*Requerimientos deseables para entender este taller:*\n",
        "* Conocimiento b√°sico de Python\n",
        "* Conocimiento b√°sico en entrenamiento de modelos\n",
        "\n",
        "*Librer√≠as principales utilizadas*:\n",
        "* [Hugging Face ü§ó ](https://huggingface.co/)\n",
        "\n",
        "*Agradecimientos*:\n",
        "* **Cristian Cardellino**, por la inspiraci√≥n en la estructura de partes del notebook. Ver [ac√°](https://colab.research.google.com/#fileId=https%3A//huggingface.co/crscardellino/xi-ciai-cba-martin-fierro/blob/main/xi-ciai-cba.ipynb) para m√°s detalles.\n",
        "* **Beatriz Busaniche**, **Nair Carolina** y **Alexia Halvorsen** de la Fundaci√≥n V√≠a Libre por su continuo apoyo.\n",
        "* **Mauricio Mazuecos** y **Edgar Altszyler**, por la organizaci√≥n de la jornada.\n",
        "* **Nicol√°s Wolovick** y a todo el equipo de **CCAD-UNC** por disponibilizar recursos para la creaci√≥n y desarrollo de este taller.\n",
        "* **FAMAF**, por la apertura y predisposici√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VnE4JoEZ1z7t",
      "metadata": {
        "id": "VnE4JoEZ1z7t"
      },
      "source": [
        "# **IMPORTANTE**\n",
        "\n",
        "* Todas las celdas indicadas con el s√≠mbolo ( ‚ùó ) son obligatorias y necesarias para el correcto funcionamiento de este taller.\n",
        "\n",
        "* Aquellas se√±aladas con ( üîé ) son de profundizaci√≥n de contenido, optativas de leer y computar.\n",
        "\n",
        "* Si una celda contiene el s√≠mbolo ( ‚è≥ ), significa que tomar√° m√°s tiempo en computarse que el promedio. Tener en cuenta para aprovechar al m√°ximo el tiempo disponible del taller."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YEo2e_80I-42",
      "metadata": {
        "id": "YEo2e_80I-42"
      },
      "source": [
        "# 0 - Inicializaci√≥n de la notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eGOTohldJe8O",
      "metadata": {
        "id": "eGOTohldJe8O"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "# Instalaci√≥n de librer√≠as\n",
        "!pip install -U transformers[torch] --quiet\n",
        "!pip install -U tabulate --quiet\n",
        "!pip install -U datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LjgyHGLMJ-TT",
      "metadata": {
        "id": "LjgyHGLMJ-TT"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "# Descarga de datasets\n",
        "import os\n",
        "\n",
        "ROOT_PATH = \"llm_adaptation_2nlp_workshop\"\n",
        "DATASETS_PATH = os.path.join(ROOT_PATH, \"datasets\")\n",
        "\n",
        "%rm -r \"$ROOT_PATH\"\n",
        "!git clone https://github.com/nanom/llm_adaptation_2nlp_workshop.git \"$ROOT_PATH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chzUFmPgLcod",
      "metadata": {
        "id": "chzUFmPgLcod"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "# Comprobaci√≥n de recursos (GPUs)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb7f3997",
      "metadata": {
        "id": "eb7f3997"
      },
      "source": [
        "# 1 - Modelos de lenguaje"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "283ca08c",
      "metadata": {
        "id": "283ca08c"
      },
      "source": [
        "## ¬øQu√© son los \"modelos de lenguaje\"?\n",
        "\n",
        "- Son modelos estad√≠sticos predictivos, basados en aprendizaje autom√°tico sobre textos.\n",
        "- Su funci√≥n principal es **analizar** y/o **generar** texto de manera autom√°tica.\n",
        "- Se entrenan encontrando patrones en grandes cantidades de texto libre.\n",
        "- Dado un contexto (e.g. una secuencia de palabras), aplican los patrones inferidos para predecir la palabra siguiente, generando texto plausible y coherente.\n",
        "- Si bien los modelos de lenguaje existen desde hace varias d√©cadas en diferentes formas (modelos markovianos, conditional random fields, redes neuronales recurrentes), actualmente cuando alguien habla de un \"Modelo de Lenguaje\", usualmente se refiere a un modelo neuronal de tipo **Transformer**.\n",
        "\n",
        "### ¬øQu√© es un **Transformer**?\n",
        "\n",
        "- Es un tipo de arquitectura de redes neuronales que se introdujo en el paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "- El tipo de aprendizaje que hacen estas redes est√° basado en **mecanismos de atenci√≥n**, que ayudan al modelo a enfocarse en las partes m√°s determinantes de la informaci√≥n, de forma que el entrenamiento sea m√°s r√°pido y menos costoso que para otras arquitecturas, como las redes neuronales recurrentes.\n",
        "\n",
        "<!-- La idea fundamental detras de un modelo Transformer, es procesar el lenguaje natural de una manera muy eficiente y efectiva, haciendolo ideal para ser aplicado a una gran cantidad de tareas de PLN.-->\n",
        "\n",
        "- Existen diferentes variantes de transformer, de acuerdo a c√≥mo y qu√© parte de sus componentes utilizan:\n",
        "    - Los modelos de traducci√≥n secuencia a secuencia o `Seq2Seq` (e.g. [T5](https://arxiv.org/abs/1910.10683)), tienen un **codificador y decodificador** y son empleados para tareas de transformaci√≥n como traducci√≥n, simplificaci√≥n, cambio de estilo o resumen.\n",
        "    - Los modelos que s√≥lo usan el **codificador** (e.g. [BERT](https://arxiv.org/abs/1810.04805)) se usan para obtener representaciones vectoriales del texto (*embeddings*) que resultan muy √∫tiles para  determinar relaciones de semejanza entre diferentes textos.\n",
        "    - Los modelos basados en el **decodificador** (e.g. [GPT](https://arxiv.org/abs/2005.14165)) se usan para generar texto autom√°ticamente, como respuestas a preguntas, ensayos o cuentos, entre otros.\n",
        "\n",
        "<center><img src='https://heidloff.net/assets/img/2023/02/transformers.png' width=60%></center>\n",
        "\n",
        "**Nota:** *Para una explicaci√≥n m√°s sencilla, pero m√°s detallada, sugiero los posts de la serie \"The Illustrated...\" de [Jay Alammar](http://jalammar.github.io/):*\n",
        "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "- [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/)\n",
        "- [The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/) / [How GPT-3 Works](http://jalammar.github.io/how-gpt3-works-visualizations-animations/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d1ca59",
      "metadata": {
        "id": "87d1ca59"
      },
      "source": [
        "## Probamos un BERT mediante la tarea *fill-mask*\n",
        "\n",
        "En esta secci√≥n vamos a practicar con [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)), un modelo de lenguaje con una arquitectura compuesta de **codificadores** (encoders).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0IFLq1qP96_l",
      "metadata": {
        "id": "0IFLq1qP96_l"
      },
      "source": [
        "Para familiarizarnos con BERT, vamos a jugar con la tarea \"*Fill-Mask*\", cuyo objetivo es completar una oraci√≥n con algunas palabras, en los lugares marcados con `[MASK]`.\n",
        "\n",
        "El modelo de lenguaje predecir√° las palabras m√°s adecuadas para encajar en esos espacios en blanco, es decir, las m√°s probables dado el contexto, seg√∫n los patrones estad√≠sticos inferidos de los ejemplos de aprendizaje. Esto nos sirve para evaluar qu√© tan bien est√° modelando los textos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viJnXm9nzbSz",
      "metadata": {
        "id": "viJnXm9nzbSz"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "# Usamos pipeline como funci√≥n auxiliar de alto nivel\n",
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    task=\"fill-mask\",\n",
        "    model=\"dccuchile/bert-base-spanish-wwm-uncased\",\n",
        "    top_k=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n8XrWlLOzyTO",
      "metadata": {
        "id": "n8XrWlLOzyTO"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "ejemplo = 'C√≥rdoba es una [MASK] de Argentina.'\n",
        "fill_mask(ejemplo)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc710ea",
      "metadata": {
        "id": "cfc710ea"
      },
      "source": [
        "## Probamos un GPT para la tarea de generaci√≥n de texto\n",
        "\n",
        "En esta secci√≥n vamos a practicar con [GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer), un modelo de lenguaje generativo con una arquitectura basada en transformers.\n",
        "\n",
        "Vamos a jugar con la funci√≥n de generaci√≥n del modelo, que, dada una secuencia de palabras, predice la continuaci√≥n m√°s probable para la misma, seg√∫n los patrones inferidos en los ejemplos de aprendizaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y4F-eNCu0D3q",
      "metadata": {
        "id": "Y4F-eNCu0D3q"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "# Usamos pipeline como funci√≥n auxiliar de alto nivel\n",
        "from transformers import pipeline\n",
        "\n",
        "text_gen = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=\"gpt2\",\n",
        "    pad_token_id=50256\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CAOGxhw10F1_",
      "metadata": {
        "id": "CAOGxhw10F1_"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "ejemplo=\"My name is Lewis and I like to\"\n",
        "text_gen(ejemplo)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8958cf42",
      "metadata": {
        "id": "8958cf42"
      },
      "source": [
        "# 2 - Librer√≠as para modelos de lenguaje\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6038bd78",
      "metadata": {
        "id": "6038bd78"
      },
      "source": [
        "##  ¬øQu√© es Hugging Face ü§ó?\n",
        "\n",
        "- Una [comunidad colaborativa](https://huggingface.co/) especialmente enfocada en modelos de lenguaje y otros recursos de Inteligencia Artificial (IA).\n",
        "- Ofrece repositorios para disponibilizar [modelos](https://huggingface.co/models), [datasets](https://huggingface.co/datasets) y [demos](https://huggingface.co/spaces).\n",
        "- Adem√°s, ofrece varias librer√≠as orientadas a la IA, particularmente al Aprendizaje Profundo (*Deep Learning*), entre las que destacan:\n",
        "    - [`transformers`](https://huggingface.co/docs/transformers): La que veremos en esta charla, para todo lo relacionado a Procesamiento del Lenguaje Natural (PLN) con grandes modelos de lenguaje (*Large Language Models*, LLMs).\n",
        "    - [`datasets`](https://huggingface.co/docs/datasets): Una librer√≠a con funcionalidades para el tratamiento de los conjuntos de datos a utilizar para entrenar o ajustar los LLMs.\n",
        "    - [`tokenizers`](https://huggingface.co/docs/tokenizers): Una librer√≠a para el proceso de \"tokenizaci√≥n\", i.e. la divisi√≥n de texto de manera discreta en palabras o subpalabras.\n",
        "- Hugging Face no s√≥lo ofrece soluciones para PLN, sino tambi√©n para im√°genes, con librer√≠as como [`diffusers`](https://huggingface.co/docs/diffusers), para la generaci√≥n de im√°genes:\n",
        "    - Lectura recomendada: [The Illustrated Stable Diffusion](http://jalammar.github.io/illustrated-stable-diffusion/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ad7d45",
      "metadata": {
        "id": "25ad7d45"
      },
      "source": [
        "## ¬øC√≥mo empezar con Hugging Face ü§ó?\n",
        "\n",
        "- Primero se [crea una cuenta en la p√°gina](https://huggingface.co/join).\n",
        "- Luego podemos [crear modelos](https://huggingface.co/new) a trav√©s del men√∫ que se despliega de nuestro avatar.\n",
        "- Para poder subir el modelo personalizado que entrenaremos en esta notebook en tu cuenta de Hugging Face, necesitar√°s generar un token de acceso mediante los siguentes pasos:\n",
        "\n",
        "> 1. **Acced√©** a la [secci√≥n de tokens de acceso](https://huggingface.co/settings/tokens) de tu perfil:\n",
        "    <center><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/User-Access-Token.png\" width=70%></center>\n",
        "\n",
        "> 2. Cre√° un **nuevo token** de acceso con permiso de escritura:\n",
        "    <center><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new-token.png\" width=60%></center>\n",
        "    \n",
        "> 3. **Listo!**. Ya ten√©s tu token preparado para poder subir tu modelo a la plataforma de Hugging Face ü§ó."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AkW7-sQW0bPf",
      "metadata": {
        "id": "AkW7-sQW0bPf"
      },
      "source": [
        "# 3 - ¬øC√≥mo se entrenan los modelos de lenguaje?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adb09645",
      "metadata": {
        "id": "adb09645"
      },
      "source": [
        "En esta notebook **NO** vamos a entrenar completamente un LLM, porque los LLMs requieren de muchos datos y mucho c√≥mputo para ser entrenados:\n",
        "- Para **BERT**:\n",
        "    - El costo total estimado de entrenamiento del modelo fue de U\\$D 6912 (para su version *large*) y de U\\$D 500 (para su versi√≥n *base*).\n",
        "    - Mientras que la versi√≥n peque√±a (*base*) cuenta con 109M de par√°metros, su versi√≥n original (*large*) oscila en 334M, m√°s del triple.\n",
        "    - Su entrenamiento fue realizado sobre 3.3B de tokens (aproximadamente 20 GB de texto no comprimido).\n",
        "\n",
        "- Para **GPT-3**:\n",
        "    - Se estim√≥ un costo de entrenamiento cercano a los U\\$D 4.6 Millones.\n",
        "    - Disponible en ocho tama√±os, que van desde los 125M a los 175B par√°metros.\n",
        "    - Requiri√≥ de varias semanas de entrenamiento.\n",
        "    - El corpus reportado en el cual fue entrenado es de aproximadamente 500B de palabras.\n",
        "    - Se necesito de varias GPUs y hardware especializado para entrenarlo.\n",
        "\n",
        "Sin embargo, vamos a **entrenar parcialmente** un modelo de lenguaje, en concreto, vamos a adaptarlo a un dominio particular.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ryVETkJgWtT3",
      "metadata": {
        "id": "ryVETkJgWtT3"
      },
      "source": [
        "\n",
        "## Diferencia entre pre-training y fine-tuning\n",
        "\n",
        "La distinci√≥n clave entre `pre-training` y `fine-tuning` se encuentra en las etapas del proceso de entrenamiento:\n",
        "\n",
        "* El **pre-training**, es la primera fase de entrenamiento al que se somete todo modelo LLM, y requiere de una enorme cantidad de textos gen√©ricos, como se ha descrito en el apartado anterior.\n",
        "\n",
        "* Una vez finalizado este proceso inicial de entrenamiento, se obtiene un modelo gen√©rico capaz de realizar predicciones gen√©ricas sobre patrones textuales, pero sin especializaci√≥n en ninguna tareas espec√≠ficas.\n",
        "\n",
        "* Si deseamos adaptar este modelo gen√©rico para que desempe√±e una tarea espec√≠fica con un alto rendimiento (e.g.: *Text Clasification*, *Sentiment analysis*, *Question Answering*, *Information Extraction*, etc.), se lleva a cabo una segunda etapa de entrenamiento conocida como **fine-tuning**.\n",
        "\n",
        "<center>\n",
        "<!-- <img src=\"https://serokell.io/files/7i/7iyrq1z5.Inside_ChatGPT_pic1.png\" width=80%>-->\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:951/0*R31A71UjHM8R8Pps.png\" width=80%>\n",
        "</center>\n",
        "<right> fuente: https://medium.com/mantisnlp/supervised-fine-tuning-customizing-llms-a2c1edbf22c3\n",
        "</right>\n",
        "\n",
        "Este entrenamiento espec√≠fico requiere de menos ejemplos de entrenamiento y menos c√°lculo, ya que s√≥lo modifica ligeramente el modelo ya entrenado. En este proceso, existen var√≠as formas en como se \"modifica\" el modelo pre-entrenado durante esta fase: se pueden modificar las √∫ltimas capas, a√±adir capas a la red pre-entrenada, y otras variantes. En esta notebook vamos a trabajar con una aproximaci√≥n sencilla, pero existen m√∫ltiples librer√≠as en Hugging Face que implementan funcionalidades para llevar adelante difrentes tipos de *fine-tuning*, como por ejemplo la popular [LoRA](https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms).\n",
        "\n",
        "<!--<center>\n",
        "<img src=\"https://media.licdn.com/dms/image/D5612AQEe2e_DJ51f2g/article-inline_image-shrink_1000_1488/0/1687324002269?e=1699488000&v=beta&t=vWDbM2RYNKa9tv5--w6Iub7YRRGkjld-s7U6VDd_7_Q\" width=80%>\n",
        "\n",
        "</center>\n",
        "<right> fuente: https://www.linkedin.com/pulse/beginners-guide-fine-tuning-large-language-models-vaidheeswaran\n",
        "</right>-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NKBq_dr38Kld",
      "metadata": {
        "id": "NKBq_dr38Kld"
      },
      "source": [
        "## *Domain Adaptation*\n",
        "\n",
        "Mientras que el *fine-tuning* se centra en ajustar un modelo de lenguaje *pre-entrenado* para realizar tareas espec√≠ficas, la `adaptaci√≥n de dominio` se enfoca en hacer que el modelo sea m√°s efectivo en un dominio de datos particular.\n",
        "\n",
        "En caso de que tus datos de entrenamiento pertenezcan a un determinado tema espec√≠fico (e.g.: leyes, medicina, inform√°tica, etc.) o un estilo espec√≠fico (e.g.: acad√©mico, infantil, publicitario, etc.), y difieran substancialmente del corpus est√°ndar en el cual fue entrenado inicialmente el LLM, podemos pensar diferentes opciones para incorparar este nuevo conocimiento:\n",
        "\n",
        "\n",
        "1. **Entrenamiento desde cero:** Este enfoque implica entrenar desde cero (*from scratch*) un nuevo modelo de lenguaje con textos del dominio de inter√©s. Sin embargo, no es recomendable para el p√∫blico en general, dada la gran cantidad de recursos computacionales, horas de procesamiento y textos de entrenamiento necesarios para lograr un rendimiento comparable a los modelos estado del arte (modelos *state-of-the-art, SOTA*).\n",
        "2.  **Uso de modelos disponibles:** Se aprovechan modelos disponibles p√∫blicamente, espec√≠ficos para el dominio de inter√©s (e.g: [LegalBERT](https://arxiv.org/abs/2010.02559), [FinBERT](https://arxiv.org/abs/1908.10063) y [BioBERT](https://arxiv.org/abs/1901.08746); todos disponibles en Hugging Face ü§ó de manera gratuita.)\n",
        "3. **Adaptaci√≥n de modelos pre-entrenados**: Esta aproximaci√≥n, tambien conocida en la literatura como *'further pre-training'*, *'inter-training'*, *'continued pre-training'* o *'domain-adaptation'*, implica tomar un modelo de lenguaje pre-entrenado y, aprovechando todo el conocimiento y las representaciones ya aprendidas, **continuar su entrenamiento sobre el conjunto de datos especializados o personalizados**. Esta t√©cnica permite alcanzar muy buenos resultados, utilizando menos recursos computacionales, horas y datos de entrenamiento.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*4eNaH6y0dSsaxtTQW0tgHQ.png\" width=80%>\n",
        "</center>\n",
        "<right> fuente: https://medium.com/@shankar.arunp/training-bert-from-scratch-on-your-custom-domain-data-a-step-by-step-guide-with-amazon-25fcbee4316a\n",
        "</right>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zWA7IwMyuQcG",
      "metadata": {
        "id": "zWA7IwMyuQcG"
      },
      "source": [
        "# 4 - Vamos a personalizar un modelo!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zv2KNF96xlXQ",
      "metadata": {
        "id": "zv2KNF96xlXQ"
      },
      "source": [
        "## 4.1 - ¬øC√≥mo personalizar un modelo BERT?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qHSuvnM21DmM",
      "metadata": {
        "id": "qHSuvnM21DmM"
      },
      "source": [
        "Tanto para el entrenamiento inicial (*pre-training*) como para la adaptaci√≥n de dominio mediante un entrenamiento intermedio (*inter-training*), el modelo debe ser re-entrenado utilizando dos tareas auto-supervisadas espec√≠ficas: **Masked Language Modeling (MLM)**, o una combinaci√≥n entre **(MLM)** y **Next Sentence Prediction (NSP)**.\n",
        "\n",
        "### Masked Language Modeling (MLM)\n",
        "> Consiste en enmascarar (ocultar) palabras aleatorias en cada oraci√≥n o frase de entrada, y entrenar el modelo para que aprenda a predecirlas bas√°ndose en el contexto circundante (palabras vecinas). Durante el entrenamiento, alrededor del **15%** de las palabras se seleccionan al azar y se enmascaran, reemplaz√°ndolas con el token especial **[MASK]** (ver la secci√≥n Enmascarado Aleatorio). De esta forma, el modelo infiere patrones de relaciones entre palabras.\n",
        "<center>\n",
        "<img src=\"http://jalammar.github.io/images/BERT-language-modeling-masked-lm.png\" width=65%>\n",
        "</center>\n",
        "\n",
        "### Next Sentence Prediction (NSP)\n",
        "> Aunque no es obligatoria, `NSP` es otro componente importante en el entrenamiento de BERT. A partir de un texto considerado como una secuencia de oraciones, se generan ejemplos de entrenamiento consistentes en pares de oraciones aleatorios. El modelo tiene que aprender a identificar si una oraci√≥n sigue a otra en el texto original. De esta forma, el modelo infiere patrones de relaciones entre oraciones.\n",
        "\n",
        "<center>\n",
        "<img src=\"http://jalammar.github.io/images/bert-next-sentence-prediction.png\" width=65%>\n",
        "</center>\n",
        "\n",
        "En esta notebook **realizaremos el ajuste de dominio mediante la tarea de MLM** utilizando la clase [**BertForMaskedLM**](https://huggingface.co/docs/transformers/v4.32.1/en/model_doc/bert#transformers.BertForMaskedLM) (modelo BERT con un bloque superior extra que posibilita el `modelado del lenguaje enmascarado`), ofrecida por la biblioteca [**transformer**](https://github.com/huggingface/transformers) de Hugging Face ü§ó.\n",
        "\n",
        "* Se utilizar√°n diferentes datasets especializados (tanto en espa√±ol como en ingl√©s).\n",
        "* Utilizaremos como modelo pre-entrenado  [bert-base-uncased](https://huggingface.co/bert-base-uncased) para realizar la adaptaci√≥n en los datasets en ingl√©s, y [dccuchile/bert-base-spanish-wwm-uncased](https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased) para los correspondientes en espa√±ol.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qHLX6wTQml1n",
      "metadata": {
        "id": "qHLX6wTQml1n"
      },
      "source": [
        "### Carga de datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rfgry7ccvHUi",
      "metadata": {
        "id": "rfgry7ccvHUi"
      },
      "source": [
        "* Utilizaremos la librer√≠a `datasets` de Hugging Face para cargar cada uno de los corpus seleccionados.\n",
        "* Para la personalizaci√≥n de BERT, tenemos un total de 4 conjuntos de datos, 3 de ellos en espa√±ol y 1 en Ingl√©s:\n",
        "    * Peppa Pig (es):\n",
        "      * ‚âà 3k muestras.\n",
        "      * Extra√≠do de subt√≠tulos de 77 episodios.\n",
        "    * Martin Fierro (es):\n",
        "      * ‚âà 2k muestras/versos.\n",
        "      * Extra√≠do del libro completo de Jos√© Hern√°ndez\n",
        "    * Preguntas de no videntes (en):\n",
        "      * ‚âà 33k muestras/preguntas.\n",
        "      * Generado a partir de preguntas visuales realizadas sobre im√°genes tomadas por personas con discapacidades visuales durante la ejecuci√≥n de sus tareas cotidianas. *(click [aqui](https://vizwiz.org/tasks-and-datasets/vqa/) para mas informaci√≥n sobre este dataset)*\n",
        "    * Rese√±as de Vinos (es):\n",
        "      * ‚âà 130k muestras.\n",
        "      * Extra√≠do de descripciones de vinos de todo el mundo.\n",
        "* A continuaci√≥n, seleccionen de la lista desplegable el dataset que mas nos interese.\n",
        "* En `max_samples`, elijan un l√≠mite m√°ximo de datos para acotar el c√≥mputo necesario.\n",
        "* Autom√°ticamente el siguiente c√≥digo dividir√° el dataset elegido en dos partes:\n",
        "    * El 80% como conjunto de entrenamiento.\n",
        "    * El 20% como conjunto de test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pSX-6B_qguzS",
      "metadata": {
        "id": "pSX-6B_qguzS"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "# @title  { run: \"auto\" }\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Fijaci√≥n de semilla para reproducibilidad de resultados\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Recolecci√≥n de par√°metros de formulario\n",
        "DATASET_NAME = \"Peppa Pig (es)\" # @param [\"Resenas de Vinos (es)\", \"Preguntas de no videntes (en)\", \"Martin Fierro (es)\", \"Peppa Pig (es)\"]\n",
        "max_samples = 30000 # @param {type:\"number\"}\n",
        "name_to_file = {\n",
        "    'Preguntas de no videntes (en)' : \"vizwiz.csv\",\n",
        "    'Resenas de Vinos (es)'         : \"wines_es.csv\",\n",
        "    'Martin Fierro (es)'            : \"martin_fierro.csv\",\n",
        "    'Peppa Pig (es)'                : \"peppa_pig.csv\",\n",
        "}\n",
        "\n",
        "# Carga de conjunto de datos\n",
        "bert_ds = load_dataset(\n",
        "    path=DATASETS_PATH,\n",
        "    data_files={'all_data': name_to_file[DATASET_NAME]},\n",
        ")\n",
        "\n",
        "# Divisi√≥n de conjunto de datos en subconjuntos de entrenamiento y testeto\n",
        "total_size = min(max_samples, len(bert_ds['all_data']))\n",
        "val_size = int(total_size *.2)\n",
        "train_size = total_size - val_size\n",
        "\n",
        "bert_ds = bert_ds[\"all_data\"].train_test_split(\n",
        "    train_size=train_size,\n",
        "    test_size=val_size,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"* Informaci√≥n de dataset '{DATASET_NAME}':\\n---\")\n",
        "bert_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WlGtTeKuqXhY",
      "metadata": {
        "id": "WlGtTeKuqXhY"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "# Vizualizaci√≥n de primeros 15 ejemplos del dataset\n",
        "for sample in bert_ds['train']['samples'][:15]:\n",
        "    print(f\">> {sample}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A6_bKfAfxlX4",
      "metadata": {
        "id": "A6_bKfAfxlX4"
      },
      "source": [
        "### Tokenizaci√≥n de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kzKg_zDnw-eN",
      "metadata": {
        "id": "kzKg_zDnw-eN"
      },
      "source": [
        "\n",
        "- La tokenizaci√≥n es un paso de pre-procesamiento esencial cuando se utiliza BERT u otro LLM.\n",
        "- BERT procesa el texto en forma de tokens, que son elementos individuales del texto de entrada, como palabras, signos de puntuaci√≥n o tokens especiales como `[CLS]`, `[SEP]`, `[PAD]`, `[UNK]` y `[MASK]`.\n",
        "- A continuaci√≥n iniciaremos y analizaremos tal proceso.\n",
        "\n",
        "Recuerde utilizar el modelo `bert-base-uncased` como modelo base para los datasets en idioma ingl√©s, y `dccuchile/bert-base-spanish-wwm-uncased` para los datasets en espa√±ol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sljDH7pDxlX4",
      "metadata": {
        "id": "sljDH7pDxlX4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "# Iniciaci√≥n de tokenizador\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "MODEL_CHECKPOINT = \"dccuchile/bert-base-spanish-wwm-uncased\" # @param [\"bert-base-uncased\",\"dccuchile/bert-base-spanish-wwm-uncased\"]\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_CHECKPOINT, TOKENIZERS_PARALLELISM=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WG5eUd-23Nna",
      "metadata": {
        "id": "WG5eUd-23Nna"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "# Tokenizaci√≥n de ejemplo de prueba\n",
        "samples = bert_ds['train']['samples'][:5]\n",
        "\n",
        "for ith, sample in enumerate(samples):\n",
        "    token_ids = tokenizer(sample)['input_ids']\n",
        "    token_str = [tokenizer.decode([tk_id]) for tk_id in token_ids]\n",
        "    print(f\"{sample}\")\n",
        "    print(f\"{token_str}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YJKX4o_wyR0P",
      "metadata": {
        "id": "YJKX4o_wyR0P"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "# Visualizaci√≥n de 'tokes especiales' utilizados por tokenizador\n",
        "for name, tk in tokenizer.special_tokens_map.items():\n",
        "    print(f\"{name}. Token: {tk}; Token_id: {tokenizer.vocab[tk]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1Q1jy5PUJjUN",
      "metadata": {
        "id": "1Q1jy5PUJjUN"
      },
      "source": [
        "* Dado que entrenaremos un modelo de BERT con un conjunto de datos que tienen una longitud variable, es fundamental aplicar el proceso de tokenizaci√≥n y el relleno (padding) adecuados a nuestros datos. Esto permite que todas las secuencias tengan la misma longitud fija, garantizando un procesamiento uniforme y eficiente de los datos durante el entrenamiento y la inferencia.\n",
        "* A continuaci√≥n realizaremos un histograma de la cantidad de tokens de cada ejemplo, y utilizaremos esa informaci√≥n para realizar posteriormente la tokenizaci√≥n del dataset completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jo2jc6IM1y9x",
      "metadata": {
        "id": "jo2jc6IM1y9x"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "# Generaci√≥n de histograma de n√∫mero de tokens por ejemplos\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# C√°lculo de n√∫mero de tokens por ejemplos\n",
        "def count_tokens_fn(batch):\n",
        "    tokenized_batch = tokenizer(batch['samples'])\n",
        "    tokenized_batch['count'] = [len(tks) for tks in tokenized_batch['input_ids']]\n",
        "    return tokenized_batch\n",
        "\n",
        "result = bert_ds.map(\n",
        "    function=count_tokens_fn,\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "num_of_tokens_list = result['train']['count'] + result['test']['count']\n",
        "\n",
        "# Generaci√≥n de histograma\n",
        "plt.hist(num_of_tokens_list, bins=35, edgecolor='k')\n",
        "plt.xlabel('N√∫mero de Tokens')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.title('N√∫mero de Tokens por Ejemplo')\n",
        "plt.show()\n",
        "\n",
        "# C√°lculo de percentiles (e.g., 25th, 50th, and 75th percentiles)\n",
        "percentiles = np.percentile(num_of_tokens_list, [25, 50, 75])\n",
        "\n",
        "print(f\"Total de ejemplos: {len(num_of_tokens_list)}\")\n",
        "print(f\"Percentil 25: {percentiles[0]}\")\n",
        "print(f\"Percentil 50 (Mediana): {percentiles[1]}\")\n",
        "print(f\"Percentile 75: {percentiles[2]}\")\n",
        "print(f\"Max: {np.max(num_of_tokens_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F0CPWdsYIFZi",
      "metadata": {
        "id": "F0CPWdsYIFZi"
      },
      "source": [
        "- A continuaci√≥n, la funci√≥n auxiliar `tokenize_data_fn` servir√° para tokenizar y codificar el conjunto completo de datos de manera eficiente mediante el [m√©todo `map`](https://huggingface.co/docs/datasets/about_map_batch).\n",
        "- Lo que devolver√° ser√° un nuevo dataset cuyos tokens estar√°n ''convertidos'' en:\n",
        "    - √≠ndices del vocabulario (*input_ids*),\n",
        "    - m√°scaras de atenci√≥n [(*attention_mask*)](https://huggingface.co/docs/transformers/glossary#attention-mask),\n",
        "    - tipos de tokens [(*token_type_ids*)](https://huggingface.co/docs/transformers/glossary#token-type-ids),\n",
        "    - y etiquetas (*labels*) que ser√°n utilizadas como `Ground Truth` en la etapa de entrenamiento.\n",
        "- `MAX_TOKEN_LENGTH`: Indica el m√°ximo n√∫mero de tokens que el modelo utilizar√° para codificar cada ejemplo/muestra. Debemos aumentarlo cuando los ejemplos de nuestro conjunto de datos son m√°s largos, pero cuanto m√°s largos son, m√°s espacio ocupan en la memoria. Una estrategia com√∫n para determinar el tama√±o m√°ximo de tokens es realizar un histograma y quedarnos con el valor del percentil 75% del largo de los ejemplos. Tambi√©n podemos utilizar el n√∫mero de tokens de la secuencia m√°s larga si disponemos de suficiente capacidad de memoria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JDyitCyOySe2",
      "metadata": {
        "id": "JDyitCyOySe2"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "# Tokenizaci√≥n de conjunto de datos completo\n",
        "\n",
        "# N√∫mero m√°ximo de tokens a utilizar\n",
        "MAX_TOKEN_LENGTH = 16 # @param {type:\"slider\", min:8, max:128, step:8}\n",
        "\n",
        "def tokenize_data_fn(batch):\n",
        "    tokenized_sample = tokenizer(\n",
        "        batch['samples'],\n",
        "        max_length=MAX_TOKEN_LENGTH,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    tokenized_sample[\"labels\"] = tokenized_sample['input_ids'].clone()\n",
        "    return tokenized_sample\n",
        "\n",
        "tokenized_bert_ds = bert_ds.map(\n",
        "    function=tokenize_data_fn,\n",
        "    batched=True,\n",
        "    remove_columns=[\"samples\"]\n",
        ")\n",
        "\n",
        "print(tokenized_bert_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "duPj1x54EQB5",
      "metadata": {
        "id": "duPj1x54EQB5"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "# Visualizaci√≥n de proceso de tokenizaci√≥n para el primer ejemplo del conjunto de datos\n",
        "sample = tokenized_bert_ds[\"train\"][0]\n",
        "\n",
        "print(f\"- input_ids:        {sample['input_ids']}\")\n",
        "print(f\"- attention_mask:   {sample['attention_mask']}\")\n",
        "print(f\"- token_type_ids:   {sample['token_type_ids']}\")\n",
        "print(f\"- labels:           {sample['labels']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jdRVPRVdHG0c",
      "metadata": {
        "id": "jdRVPRVdHG0c"
      },
      "source": [
        "### Enmascarado Aleatorio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h_ShVU5yQIth",
      "metadata": {
        "id": "h_ShVU5yQIth"
      },
      "source": [
        "> Este proceso es el encargado de ocultar aleatoriamente el 15\\% de los tokens y formatear los datos de entrenamiento de manera adecuada, asegurando que las secuencias de entrada tengan la longitud correcta y que las m√°scaras de atenci√≥n se apliquen correctamente.\n",
        "\n",
        "* Para implementar este proceso de manera sencilla, utilizaremos el m√©todo [DataCollatorForLanguageModeling](https://huggingface.co/docs/transformers/v4.32.1/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling) proporcionado por la biblioteca de Hugging Face.\n",
        "\n",
        "* Como se podr√° observar en el siguente ejemplo, es com√∫n utilizar un valor especial como **-100** en la salida de las etiquetas (*labels*) para indicar la posici√≥n de los tokens que se **IGNORAN** en el c√°lculo de la funci√≥n de p√©rdida llevada a cabo durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dWHvQlnUHBWb",
      "metadata": {
        "id": "dWHvQlnUHBWb"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "import torch, random\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collate_fn = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G9xeuMP5H1aq",
      "metadata": {
        "id": "G9xeuMP5H1aq"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "# Visualizacion de proceso de 'enmascarado aleatorio' de tokens\n",
        "seed = random.randint(0,100)\n",
        "samples = [tokenized_bert_ds['train'][i] for i in range(3)]\n",
        "\n",
        "print(\"DataCollator Outputs\")\n",
        "torch.manual_seed(seed)\n",
        "print(\"--- Tokens:\")\n",
        "for sample in data_collate_fn(samples)['input_ids']:\n",
        "    print(f\">> {[tokenizer.decode([tk]) for tk in sample]}'\")\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "print(\"\\n--- 'input_ids':\")\n",
        "for sample in data_collate_fn(samples)['input_ids']:\n",
        "    print(f\">> {sample.tolist()}'\")\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "print(\"\\n--- 'labels':\")\n",
        "for sample in data_collate_fn(samples)['labels']:\n",
        "    print(f\">> {sample.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XiEflsvRxlX1",
      "metadata": {
        "id": "XiEflsvRxlX1"
      },
      "source": [
        "### Probando el Modelo Base"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tSwnO4nXvWU7",
      "metadata": {
        "id": "tSwnO4nXvWU7"
      },
      "source": [
        "- Antes de **personalizar** (adaptar) el modelo a nuestro conjunto de datos seleccionado, veremos c√≥mo se desenvuelve en la tarea de *fill-mask*.\n",
        "\n",
        "- Para ello, escriba una frase muy caracter√≠stica del dominio al que se quiere adaptar, y enmascare alguna de sus palabras utilizando el token `[MASK]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NL3CDtZQxlX2",
      "metadata": {
        "id": "NL3CDtZQxlX2"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "# Ejemplo para espa√±ol, pensando en adaptar el modelo a Peppa Pig\n",
        "# Usamos pipeline como funci√≥n auxiliar de alto nivel\n",
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    task=\"fill-mask\",\n",
        "    model=\"dccuchile/bert-base-spanish-wwm-uncased\",\n",
        "    top_k=5\n",
        ")\n",
        "\n",
        "ejemplo = 'A Peppa le encanta saltar en los [MASK] de barro' # @param {type:\"string\"}\n",
        "fill_mask(ejemplo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-RIWajZ9AESD",
      "metadata": {
        "id": "-RIWajZ9AESD"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "# Ejemplo para ingl√©s, pensando en adaptar el modelo a VizWiz\n",
        "# Usamos pipeline como funci√≥n auxiliar de alto nivel\n",
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    task=\"fill-mask\",\n",
        "    model=\"bert-base-uncased\",\n",
        "    top_k=5\n",
        ")\n",
        "\n",
        "ejemplo = \"Can you [MASK] me the model on the back of the iPhone?\" # @param {type:\"string\"}\n",
        "fill_mask(ejemplo)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dS8OpI86xlX6",
      "metadata": {
        "id": "dS8OpI86xlX6"
      },
      "source": [
        "### Entrenamiento para adaptaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8rwbTbV6YqfY",
      "metadata": {
        "id": "8rwbTbV6YqfY"
      },
      "source": [
        "- Una vez definido el conjunto de datos y completado el proceso de tokenizaci√≥n, pasemos a la parte m√°s intensa computacionalmente, el entrenamiento.\n",
        "- A continuaci√≥n, definimos las configuraciones del entrenamiento mediante [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.TrainingArguments).\n",
        "- ... y definimos el entrenamiento del modelo mediante la clase  [`Trainer`](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer).\n",
        "    \n",
        "> <u>Detalles de par√°metros a configurar</u>:\n",
        "1.   ***epochs***: N√∫mero de √©pocas de entrenamiento. En cada epoch, el algoritmo de entrenamiento recibe cada uno de los ejemplos del dataset de entrenamiento.\n",
        "2.  ***batch_size***: Cantidad de datos procesados por iteraci√≥n antes de actualizar el modelo (aumentar este n√∫mero mejora las estimaciones de cada iteraci√≥n ya que ser√° una muestra m√°s representativa del dataset en general, pero requerir√° m√°s memoria y ser√° necesario monitorizar y re-ajustar el `learning_rate`)\n",
        "3.  ***learning_rate***:  Tama√±o del ''ajuste'' realizado en el modelo en cada iteraci√≥n, mientras se avanza hacia un m√≠nimo de la funci√≥n de p√©rdida.\n",
        "\n",
        "**Nota**: El valor de estos par√°metros, en conjunci√≥n al poder de c√≥mputo disponible, determinar√° el tiempo y la cantidad de c√°lculo requerido para poder realizar el entrenamiento del modelo. Puede ir desde unos segundos hasta varios minutos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3NP1ORPxlX7",
      "metadata": {
        "id": "c3NP1ORPxlX7"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "from transformers import TrainingArguments, Trainer, BertForMaskedLM\n",
        "\n",
        "# Eliminaci√≥n de checkpoints anteriores\n",
        "!rm -r \"$CUSTOM_MODEL_CHECKPOINT\"\n",
        "\n",
        "# Configuraci√≥n de hiperpar√°metros\n",
        "MODEL_CHECKPOINT = \"dccuchile/bert-base-spanish-wwm-uncased\" # @param [\"dccuchile/bert-base-spanish-wwm-uncased\", \"bert-base-uncased\"]\n",
        "BATCH_SIZE = 64 # @param {type:\"slider\", min:8, max:64, step:8}\n",
        "LEARNING_RATE = 2e-5 # @param {type:\"number\"}\n",
        "EPOCHS = 5 # @param {type:\"slider\", min:3, max:15, step:1}\n",
        "LOGGING_STEPS = len(tokenized_bert_ds[\"train\"]) // BATCH_SIZE\n",
        "CUSTOM_MODEL_CHECKPOINT = \"bert_adaptation_\" + DATASET_NAME.replace(\" \", \"_\")[:-5].lower()\n",
        "\n",
        "# Configuraci√≥n de entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=CUSTOM_MODEL_CHECKPOINT,\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    num_train_epochs=EPOCHS,\n",
        "    optim='adamw_torch',\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    push_to_hub=False,\n",
        "    fp16=True,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    report_to=\"none\",\n",
        "    save_strategy=\"no\"\n",
        ")\n",
        "\n",
        "# Instanciaci√≥n de modelo\n",
        "model = BertForMaskedLM.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# Instanciaci√≥n de clase Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_bert_ds[\"train\"],\n",
        "    eval_dataset=tokenized_bert_ds[\"test\"],\n",
        "    data_collator=data_collate_fn,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.args._n_gpu = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CcqCvC7kjC0C",
      "metadata": {
        "id": "CcqCvC7kjC0C"
      },
      "source": [
        "* Antes de comenzar el entrenamiento, calcularemos la [**Perplexity**](https://huggingface.co/docs/transformers/perplexity) del modelo de lenguaje.\n",
        "\n",
        "> La `Perplexity` es una m√©trica que nos permite evaluar cu√°nta incertidumbre tiene un modelo cuando tiene que predecir palabras en un conjunto de prueba determinado. La misma, ayuda a medir qu√© tan bien conoce el modelo el lenguaje, y cu√°n coherentes ser√°n sus predicciones. En general, una *perplejidad* m√°s baja indicar√° que el modelo genera textos que claramente pertenecen al lenguaje.\n",
        "<center>\n",
        "<img src=\"https://thegradient.pub/content/images/size/w1600/2020/04/xkcd_entropy-2.png\" width=50%>\n",
        "</center>\n",
        "fuente: https://thegradient.pub/understanding-evaluation-metrics-for-language-models/\n",
        "\n",
        "Si queremos evaluar qu√© tanto se adapta un modelo a un dominio determinado, podemos estimar su perplejidad para las tareas de completar palabras (MLM) o generaci√≥n de texto en oraciones del dominio de inter√©s, ya sea sobre conjunto de oraciones que nos interesen particularmente (un *benchmark* o *testbed*) o sobre un  corpus representativo del dominio en general.\n",
        "\n",
        "> Los animamos a crear un mini-testbed, una listita de oraciones que les permita ver r√°pidamente qu√© pasa con diferentes tipos de adaptaciones. Les va a servir mucho para monitorear de forma sistem√°tica los cambios en su sistema! Luego pueden compartir sus testbeds con la comunidad, un buen testbed vale oro :-)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VIhYYrmgO6ac",
      "metadata": {
        "id": "VIhYYrmgO6ac"
      },
      "outputs": [],
      "source": [
        "# @markdown  üîé\n",
        "import math\n",
        "\n",
        "# Fijamos semilla para reproducibilidad de resultados\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Estimaci√≥n de perplexity previa a entrenamieto\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ypbXFA8TaCeh",
      "metadata": {
        "id": "ypbXFA8TaCeh"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó‚è≥\n",
        "# Inicio de entrenamiento\n",
        "trainer.train()\n",
        "\n",
        "# Guardado de entrenamiento\n",
        "trainer.save_model(CUSTOM_MODEL_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gjVvLE_6OUOE",
      "metadata": {
        "id": "gjVvLE_6OUOE"
      },
      "source": [
        "\n",
        "* Ahora, estimemos nuevamente la `perplexity` del modelo y comparemos con el valor obtenido previamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uWAZ_qTzb_AA",
      "metadata": {
        "id": "uWAZ_qTzb_AA"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "# Fijamos semilla para reproducibilidad de resultados\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Estimaci√≥n de perplexity posterior a entrenamiento\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aOUDT3zYxlX7",
      "metadata": {
        "id": "aOUDT3zYxlX7"
      },
      "source": [
        "\n",
        "### Probando el Modelo Adaptado\n",
        "\n",
        "- Ahora que tenemos el modelo entrenado con el dataset espec√≠fico, la pregunta es, ¬øC√≥mo se comportar√°?\n",
        "- Para ello volvemos a hacer la prueba anterior utilizando la tarea de *fill-mask*, quiz√°s esta vez con resultados m√°s ajustados al dominio. Ac√° es donde se luce un buen testbed!\n",
        "- Para que los resultados sean mas f√°ciles de comparar, la funci√≥n `compare_models()` ser√° la encargada de procesar y retornar, para cada orai√≥n de ejemplo ingresada, un ranking comparativo de las predicciones realizadas entre el nuevo modelo adaptado y el modelo base original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VvR2XoSGxlX7",
      "metadata": {
        "id": "VvR2XoSGxlX7"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "import os\n",
        "import transformers\n",
        "from typing import List\n",
        "from tabulate import tabulate\n",
        "from transformers import pipeline\n",
        "\n",
        "# Funci√≥n auxiliar para comparaci√≥n de predicciones de modelos\n",
        "def compare_models(\n",
        "    custom_model: transformers.pipelines.fill_mask.FillMaskPipeline,\n",
        "    base_model: transformers.pipelines.fill_mask.FillMaskPipeline,\n",
        "    sentences: List[str],\n",
        "    top_k=5\n",
        ") -> None:\n",
        "\n",
        "    headers = ['Sent','Custom Bert', '(%)', 'Base Bert', '(%)']\n",
        "    rows = []\n",
        "    for sent in sentences:\n",
        "        rows.append([sent, '', '', '', ''])\n",
        "\n",
        "        c_out = custom_model(sent)\n",
        "        o_out = base_model(sent)\n",
        "\n",
        "        c_tokens = [(i['token_str'], str(round(i['score']*100,1))+\"%\") for i in c_out]\n",
        "        o_tokens = [(i['token_str'], str(round(i['score']*100,1))+\"%\") for i in o_out]\n",
        "\n",
        "        for (p1,s1), (p2,s2) in zip(c_tokens, o_tokens):\n",
        "            rows.append(['', p1, s1, p2, s2])\n",
        "\n",
        "    table = tabulate(rows, headers=headers, tablefmt=\"text\", numalign=\"center\")\n",
        "    print(table, \"\\n\")\n",
        "\n",
        "# Instanciaci√≥n de modelo personalizado y modelo base\n",
        "base_model = pipeline(task=\"fill-mask\", model=MODEL_CHECKPOINT)\n",
        "custom_model = pipeline(task=\"fill-mask\", model=CUSTOM_MODEL_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JTAxPD4qd-ZG",
      "metadata": {
        "id": "JTAxPD4qd-ZG"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "\n",
        "# Conjunto de ejemplos de prueba\n",
        "vizwiz_test_examples = [\n",
        "    \"can you tell me the title of the book? [MASK].\",\n",
        "    \"please [MASK] this shirt\",\n",
        "    \"what [MASK] is this?\",\n",
        "    \"[MASK] can you tell me what is this?\",\n",
        "    \"how clear is this [MASK]?\",\n",
        "    \"is the floor [MASK]?\",\n",
        "    \"the microwave is [MASK]?.\"\n",
        "]\n",
        "\n",
        "vinos_test_examples = [\n",
        "    \"Este [MASK] argentino de altura es una verdadera\",\n",
        "    \"Con un color profundo e [MASK]\",\n",
        "    \"Los sabores de [MASK] persisten\",\n",
        "    \"Hecho 100% de [MASK]\",\n",
        "    \"El sabor fue [MASK]\",\n",
        "    \"Con su [MASK] mezcla de minerales\",\n",
        "    \"Por primera vez en [MASK],\",\n",
        "    \"Brazamora de [MASK] bonito\"\n",
        "]\n",
        "\n",
        "martin_fierro_test_examples = [\n",
        "    \"En los campos de la [MASK] extensa\",\n",
        "    \"donde el viento y el sol se [MASK]\",\n",
        "    \"cabalgo firme con mi [MASK]\",\n",
        "    \"bajo el cielo de [MASK] risue√±o\",\n",
        "\n",
        "    \"Con mi [MASK] y mi lazo\",\n",
        "    \"enfrento a la [MASK] con valor\",\n",
        "    \"bati√©ndome siempre a mi [MASK]\",\n",
        "    \"en este ancho y rudo [MASK]\",\n",
        "\n",
        "    \"As√≠ cabalgo con alma [MASK]\",\n",
        "    \"como otros en su [MASK]\",\n",
        "    \"defendiendo mi [MASK] y mi gente\",\n",
        "    \"en esta tierra que [MASK] sin cesar\",\n",
        "\n",
        "    \"Con [MASK] al cinto, firme y diestro\",\n",
        "    \"Enfrento [MASK], firme como el resto\",\n",
        "    \"Mate en mano, bajo el cielo [MASK]\",\n",
        "    \"Mi coraje y mi [MASK], jam√°s han claudicado.\"\n",
        "]\n",
        "\n",
        "peppa_pig_test_examples = [\n",
        "    \"[MASK], puedes decir dinosarurio?\",\n",
        "    \"¬°Hola, soy Peppa [MASK]!\",\n",
        "    \"George tiene un [MASK] muy querido.\",\n",
        "    \"El sol brilla en el [MASK] del jard√≠n.\",\n",
        "    \"Peppa y George juegan en el [MASK].\",\n",
        "    \"Mam√° Pig prepara [MASK] deliciosos.\",\n",
        "    \"Vamos al [MASK] para montar en bicicleta.\",\n",
        "    \"El [MASK] est√° lleno de flores hermosas.\",\n",
        "    \"Peppa y sus amigos tienen [MASK] en el parque.\"\n",
        "]\n",
        "\n",
        "# Moficar 'sentences' acorde al conjunto de datos utilizado para la personalizaci√≥n\n",
        "compare_models(\n",
        "    custom_model=custom_model,\n",
        "    base_model=base_model,\n",
        "    sentences=peppa_pig_test_examples\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AlaINEj1o2LR",
      "metadata": {
        "id": "AlaINEj1o2LR"
      },
      "source": [
        "### Subiendo tu modelo a Hugging Face  ü§ó"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad07BcWo7x3",
      "metadata": {
        "id": "3ad07BcWo7x3"
      },
      "source": [
        "Quer√©s que tu familia, amigos y futuros empleadores puedan probar tu chatbot? En Huggingface ü§ó pod√©s desplegar una demo muy f√°cilmente:\n",
        "\n",
        "> Recorda generar tu **token de acceso** siguendo las instrucciones de la [Secci√≥n 2](#Librer√≠as-para-modelos-de-lenguaje). Ser√° necesario para que puedas autentificarte en la plataforma y subir tu modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D-X7gPJzo5vX",
      "metadata": {
        "id": "D-X7gPJzo5vX"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "# Logeo a plataforma mediante token de acceso\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RDnfp1vSo6-s",
      "metadata": {
        "id": "RDnfp1vSo6-s"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "# Esto env√≠a el modelo entrenado al repositorio de modelos de Hugging Face.\n",
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NJ4bJC4VPwCi",
      "metadata": {
        "id": "NJ4bJC4VPwCi"
      },
      "source": [
        "## 4.2 - ¬øC√≥mo personalizar un modelo GPT?\n",
        "\n",
        "\n",
        "- Se inicia por alg√∫n modelo pre-entrenado para la tarea espec√≠fica que uno busca (e.g. clasificaci√≥n, generaci√≥n, etc).\n",
        "- Se toma un corpus especializado (anotado, revisado, etc.) y se entrena utilizando dicho corpus.\n",
        "- Intentaremos [entrenar que un modelo genere texto](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb) con el estilo de alguno de los conjuntos de datos disponibles para este taller.\n",
        "- Utilizaremos el modelo `DeepESP/gpt2-spanish` como base para espa√±ol y el modelo `gpt2` como base para ingl√©s.\n",
        "* A diferencia de la adaptaci√≥n de dominio realizada en el modelo BERT, los modelos de generaci√≥n de texto se entrenan utilizando una tarea auto-supervisada denominada [*Causal Language Modeling*](https://huggingface.co/docs/transformers/tasks/language_modeling).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YWO2pU8Drrkv",
      "metadata": {
        "id": "YWO2pU8Drrkv"
      },
      "source": [
        "### Carga de Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9LKlbQIhPwCx",
      "metadata": {
        "id": "9LKlbQIhPwCx"
      },
      "source": [
        "* Utilizaremos la librer√≠a `datasets` de HuggingFace para cargar cada uno de los corpus seleccionados.\n",
        "* Para la personalizaci√≥n de GPT, tenemos un total de 6 conjuntos de datos, todos en espa√±ol:\n",
        "    * Julio Cort√°zar (es):\n",
        "      * ‚âà 14k oraciones/muestras.\n",
        "      * Extra√≠do de una selecci√≥n de libros y cuentos del autor argentino.\n",
        "    * Edgar Allan Poe (es):\n",
        "      * ‚âà 48k oraciones/muestras.\n",
        "      * Extra√≠do de una selecci√≥n de libros y cuentos del autor estadounidense traducidos al espa√±ol.\n",
        "    * Jos√© Saramago (es):\n",
        "      * ‚âà 105k oraciones/muestras.\n",
        "      * Extra√≠do de una selecci√≥n de libros y cuentos del autor portugu√©s traducidos al espa√±ol.\n",
        "    * Peppa Pig (es):\n",
        "      * ‚âà 3k oraciones/muestras.\n",
        "      * Extra√≠do de subt√≠tulos de 77 episodios.\n",
        "    * Martin Fierro (es):\n",
        "      * ‚âà 2k estrofas/muestras.\n",
        "      * Extra√≠do del libro completo de Jos√© Hern√°ndez\n",
        "    * Rese√±as de Vinos (es):\n",
        "      * ‚âà 130k oraciones/muestras.\n",
        "      * Extra√≠do de descripciones de vinos de todo el mundo.\n",
        "\n",
        "* A continuaci√≥n, seleccionen de la lista desplegable el dataset que mas nos interese.\n",
        "* En `max_samples`, elijan un l√≠mite m√°ximo de datos para acotar el c√≥mputo necesario.\n",
        "* Autom√°ticamente el siguiente c√≥digo dividir√° el dataset elegido en dos partes:\n",
        "    * El 80% como conjunto de entrenamiento.\n",
        "    * El 20% como conjunto de test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1P7fdicp9j3E",
      "metadata": {
        "id": "1P7fdicp9j3E"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "# @title  { run: \"auto\" }\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Fijamos semilla para reproducibilidad de resultados\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Recolecci√≥n de par√°metros de formulario\n",
        "dataset_name = \"Martin Fierro (es)\" # @param [\"Resenas de Vinos (es)\", \"Martin Fierro (es)\", \"Peppa Pig (es)\", \"Julio Cortazar (es)\", \"Edgar Allan Poe (es)\", \"Jose Saramago (es)\"]\n",
        "max_samples = 30000 # @param {type:\"number\"}\n",
        "name_to_file = {\n",
        "    'Resenas de Vinos (es)'         : \"wines_es.csv\",\n",
        "    'Martin Fierro estrofas (es)'   : \"martin_fierro_v2.csv\",\n",
        "    'Martin Fierro versos (es)'     : \"martin_fierro.csv\",\n",
        "    'Peppa Pig (es)'                : \"peppa_pig.csv\",\n",
        "    'Julio Cortazar (es)'           : \"cortazar.csv\",\n",
        "    'Edgar Allan Poe (es)'          : \"poe.csv\",\n",
        "    'Jose Saramago (es)'            : \"saramago.csv\",\n",
        "}\n",
        "\n",
        "# Carga de conjunto de datos\n",
        "gpt2_ds = load_dataset(\n",
        "    path=DATASETS_PATH,\n",
        "    data_files={'all_data': name_to_file[dataset_name]},\n",
        ")\n",
        "\n",
        "# Divisi√≥n de conjunto de datos en subconjuntos de entrenamiento y testeto\n",
        "total_size = min(max_samples, len(gpt2_ds['all_data']))\n",
        "val_size = int(total_size *.2)\n",
        "train_size = total_size - val_size\n",
        "\n",
        "gpt2_ds = gpt2_ds[\"all_data\"].train_test_split(\n",
        "    train_size=train_size,\n",
        "    test_size=val_size,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"* Informaci√≥n del dataset '{dataset_name}':\\n---\")\n",
        "gpt2_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QiiUUp1o_e8N",
      "metadata": {
        "id": "QiiUUp1o_e8N"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "# Visualizaci√≥n de primeros 15 ejemplos del dataset\n",
        "for sample in gpt2_ds['train']['samples'][:15]:\n",
        "    print(f\">> {sample}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RDWpWGNuPwCs",
      "metadata": {
        "id": "RDWpWGNuPwCs"
      },
      "source": [
        "### Probando el Modelo Base\n",
        "\n",
        "- Antes de ajustar el modelo veamos c√≥mo se desenvuelve en la tarea de generaci√≥n de texto al proporcionarle como entrada la frase:\n",
        "> `Ayer, al despertarme `\n",
        "\n",
        "Luego del entrenamiento, volveremos a comparar su estilo de generaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7V3QxDyzPwCr",
      "metadata": {
        "id": "7V3QxDyzPwCr"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "BASE_MODEL = \"DeepESP/gpt2-spanish\" # @param [\"DeepESP/gpt2-spanish\", \"gpt2\"]\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6PYTSVt0PwCv",
      "metadata": {
        "id": "6PYTSVt0PwCv"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "\n",
        "# Fijamos semilla para reproducibilidad de resultados\n",
        "torch.manual_seed(42)\n",
        "\n",
        "input_ids = tokenizer.encode(\"Ayer, al depertarme\", return_tensors='pt')\n",
        "sampling_output = model.generate(input_ids, do_sample=True, max_length=50, top_k=50, top_p=0.95, pad_token_id=tokenizer.eos_token_id)\n",
        "output = tokenizer.decode(sampling_output[0], skip_special_tokens=True)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RFCk1XZEPwC5",
      "metadata": {
        "id": "RFCk1XZEPwC5"
      },
      "source": [
        "### Tokenizando los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EU_OrPFP0Oot",
      "metadata": {
        "id": "EU_OrPFP0Oot"
      },
      "source": [
        "\n",
        "- Al igual que en la primera parte de este taller, tokenizaremos el dataset.\n",
        "- Realizaremos un histograma de la frecuencia de tokens por ejemplo, y utilizaremos esta informaci√≥n para posteriormente realizar la tokenizaci√≥n del dataset completo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gW1ZBper0F-4",
      "metadata": {
        "id": "gW1ZBper0F-4"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "# Generaci√≥n de histograma del n√∫mero de tokens por ejemplos\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# C√°lculo de n√∫mero de tokens por ejemplos\n",
        "def count_tokens_fn(batch):\n",
        "    tokenized_batch = tokenizer(batch['samples'])\n",
        "    tokenized_batch['count'] = [len(tks) for tks in tokenized_batch['input_ids']]\n",
        "    return tokenized_batch\n",
        "\n",
        "result = gpt2_ds.map(\n",
        "    function=count_tokens_fn,\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "num_of_tokens_list = result['train']['count'] + result['test']['count']\n",
        "\n",
        "# Generaci√≥n de histograma\n",
        "plt.hist(num_of_tokens_list, bins=35, edgecolor='k')\n",
        "plt.xlabel('N√∫mero de Tokens')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.title('N√∫mero de Tokens por Ejemplo')\n",
        "plt.show()\n",
        "\n",
        "# C√°lculo de percentiles (e.g., 25th, 50th, and 75th percentiles)\n",
        "percentiles = np.percentile(num_of_tokens_list, [25, 50, 75])\n",
        "\n",
        "print(f\"Total de ejemplos: {len(num_of_tokens_list)}\")\n",
        "print(f\"Percentil 25: {percentiles[0]}\")\n",
        "print(f\"Percentil 50 (Mediana): {percentiles[1]}\")\n",
        "print(f\"Percentile 75: {percentiles[2]}\")\n",
        "print(f\"Max: {np.max(num_of_tokens_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eIi-I4dP0Hrb",
      "metadata": {
        "id": "eIi-I4dP0Hrb"
      },
      "source": [
        "- Al igual que antes, en `MAX_TOKEN_LENGTH` podr√°n elegir el largo m√°ximo que el modelo utilizar√° para codificar cada ejemplo/muestra de entrada. Truncando aquellos que superen este l√≠mite, y realizando un padding para homogeneizar la longitud de las muestras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M0FrWUgDPwC5",
      "metadata": {
        "id": "M0FrWUgDPwC5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "# Tokenicaci√≥n de conjunto de datos completo\n",
        "\n",
        "# N√∫mero m√°ximo de tokens a utilizar\n",
        "MAX_TOKEN_LENGTH = 30 # @param {type:\"slider\", min:8, max:128, step:8}\n",
        "\n",
        "def tokenize_data_fn(batch):\n",
        "    tokenized_sample = tokenizer(\n",
        "        batch['samples'],\n",
        "        max_length=MAX_TOKEN_LENGTH,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    tokenized_sample[\"labels\"] = tokenized_sample['input_ids'].clone()\n",
        "    return tokenized_sample\n",
        "\n",
        "tokenized_gpt2_ds = gpt2_ds.map(\n",
        "    function=tokenize_data_fn,\n",
        "    batched=True,\n",
        "    remove_columns=[\"samples\"]\n",
        ")\n",
        "\n",
        "print(tokenized_gpt2_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q8Xg_DgOPwC9",
      "metadata": {
        "id": "q8Xg_DgOPwC9"
      },
      "source": [
        "### Decodificando\n",
        "\n",
        "- Podemos ver que los textos pasan a estar agrupados en bloques de `MAX_TOKEN_LENGTH` tokens.\n",
        "- Adem√°s, vemos que el texto fue reemplazado por n√∫meros (√≠ndices en el vocabulario).\n",
        "- Por √∫ltimo, si decodificamos estos n√∫meros, obtenemos el texto original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ivXS3A6PwC9",
      "metadata": {
        "id": "2ivXS3A6PwC9"
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "print(len(tokenized_gpt2_ds['train'][0]['input_ids']))\n",
        "print(tokenized_gpt2_ds['train'][0]['input_ids'][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CDT-FvqGPwC-",
      "metadata": {
        "id": "CDT-FvqGPwC-",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# @markdown üîé\n",
        "print(tokenizer.decode(tokenized_gpt2_ds['train'][0]['input_ids']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Imydgb_2PwC_",
      "metadata": {
        "id": "Imydgb_2PwC_"
      },
      "source": [
        "### Entrenamiento\n",
        "\n",
        "- Una vez definido el conjunto de datos, pasamos a la parte m√°s intensa computacionalmente, el entrenamiento.\n",
        "- Podemos decidir guardar el modelo localmente o hacer un backup de cada √©poca del modelo en Hugging Face.\n",
        "- Definimos las propiedades del entrenamiento mediante [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.TrainingArguments).\n",
        "- Definimos el entrenamiento del modelo mediante [`Trainer`](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer).\n",
        "    - El entrenamiento tardar√° desde unos segundos hasta varias horas dependiendo el poder de c√≥mputo, par√°metros elegidos y tama√±o del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PastX4Q8PwDA",
      "metadata": {
        "id": "PastX4Q8PwDA"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó‚è≥\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "# Configuraci√≥n de funci√≥n data_collator\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Configuraci√≥n de hiperpar√°metros\n",
        "LEARNING_RATE = 2e-5 # @param {type:\"number\"}\n",
        "BATCH_SIZE = 8 # @param {type:\"slider\", min:8, max:64, step:8}\n",
        "EPOCHS = 4 # @param {type:\"slider\", min:3, max:15, step:1}\n",
        "model_name = dataset_name.replace(\" \", \"-\")[:-5]\n",
        "\n",
        "# Configuraci√≥n de entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    model_name,\n",
        "    evaluation_strategy='epoch',\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=5\n",
        ")\n",
        "\n",
        "# Instanciaci√≥n de modelo y clase Trainer\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_gpt2_ds['train'],\n",
        "    eval_dataset=tokenized_gpt2_ds['test'],\n",
        "    data_collator=data_collate_fn\n",
        ")\n",
        "\n",
        "# Inicio de entrenamiento\n",
        "trainer.args._n_gpu = 1\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sx_77z2zVFGk",
      "metadata": {
        "id": "sx_77z2zVFGk"
      },
      "source": [
        "- Ahora ser√° necesario ingresar a la plataforma HuggingFace ü§ó para subir nuestro modelo. Luego podr√°n utilizarlo tanto en este taller con la librer√≠a `transformers` como compartir con sus amigos en la versi√≥n web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FhV7SE2SPwDB",
      "metadata": {
        "id": "FhV7SE2SPwDB"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "\n",
        "# Logeo a plataforma mediante token de acceso\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wOknhZZ9PwDD",
      "metadata": {
        "id": "wOknhZZ9PwDD"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "\n",
        "# Esto env√≠a el modelo entrenado al repositorio de modelos de Hugging Face.\n",
        "trainer.push_to_hub()\n",
        "tokenizer.push_to_hub(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E0cGM72OPwDE",
      "metadata": {
        "id": "E0cGM72OPwDE"
      },
      "source": [
        "### Probando el Nuevo Modelo\n",
        "\n",
        "- Ahora que tenemos el modelo entrenado, la pregunta es, ¬øC√≥mo se comportar√°?\n",
        "- Para ello volvemos a hacer la prueba anterior, quiz√°s esta vez con mejores resultados.\n",
        "- En `MODEL` deber√°n pegar el link al modelo que acaban de subir. Tendr√° un formato del estilo `username/modelName`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xAlb6ROyPwDF",
      "metadata": {
        "id": "xAlb6ROyPwDF"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "MODEL = \"guidoivetta/Peppa-Pig\" # @param {type:\"string\"}\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mFQwGpKQWVIz",
      "metadata": {
        "id": "mFQwGpKQWVIz"
      },
      "source": [
        "- En esta celda podr√°n probar su modelo. Les incentivamos a ser creativos con las frases que elijan para completarla y comparar los resultados del modelo base con el modelo adaptado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sinm67n3PwDH",
      "metadata": {
        "id": "sinm67n3PwDH"
      },
      "outputs": [],
      "source": [
        "# @markdown ‚ùó\n",
        "\n",
        "ejemplo = \"Ayer, al despertarme\" # @param {type:\"string\"}\n",
        "input_ids = tokenizer.encode(ejemplo, return_tensors='pt')\n",
        "sampling_output = model.generate(input_ids, do_sample=True, max_length=50, top_k=50, top_p=0.95, pad_token_id=tokenizer.eos_token_id)\n",
        "output = tokenizer.decode(sampling_output[0], skip_special_tokens=True)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dscWR-DCGhVV",
      "metadata": {
        "id": "dscWR-DCGhVV"
      },
      "source": [
        "> Este trabajo est√° licenciado bajo [Licencia MIT](https://github.com/nanom/llm_adaptation_2nlp_workshop/blob/main/LICENSE)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "rise": {
      "scroll": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
